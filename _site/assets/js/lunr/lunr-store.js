var store = [{
        "title": "(Rand) ANNagram",
        "excerpt":"For my first project in this blog I write from scratch a pointer network with a transformer backbone. For smaller inputs its a silly idea because exhaustive checking would be quick and accurate. That approach though would in a combinatoric fashion. On the other hand a transformer-based encoder/decoder will grow...","categories": [],
        "tags": ["transformer","pointer network","fun"],
        "url": "/ANNagram/",
        "teaser":null},{
        "title": "(Res) Intermediate Loss Sampling, a Differentiable Categorical Sampler",
        "excerpt":"I propose a novel sampling approach, leveraging an intermediate loss function to differentiate through a categorical draw. There exists a long history of using policy gradient techniques where only the policy network gradients are utilized, but in the last couple of years approaches like the Gumbel Softmax has surfaced. Gumbel...","categories": [],
        "tags": ["differentiable sampling","research"],
        "url": "/ILSampling/",
        "teaser":null},{
        "title": "(Res) Focal Gradient Loss: Are we looking at focal loss correctly?",
        "excerpt":"Retinanet is a near state-of-the-art object detector that using a simple adaptive weighting scheme, helps bridge some of the gap between one and two stage object detectors by dealing with inherent class imbalance from the large background set constructed by the anchoring process. Specifically they use Focal Loss The usage...","categories": [],
        "tags": ["optimization","object detection","research"],
        "url": "/FocalGradLoss/",
        "teaser":null},{
        "title": "(Pers) Hack Umass Mentor",
        "excerpt":"This past weekend I had the pleasure of being a mentor at Hack Umass. Going to Umass as an ECE student, I saw the development of this event from a small hardware-only hackathon to what it is now. Now every year, me and a few friends drive up to the...","categories": [],
        "tags": ["hackathon","personal"],
        "url": "/HackUmassMentor/",
        "teaser":null},{
        "title": "(Res) Super-Masks: Are They Good Initializations?",
        "excerpt":"A large advancement in understanding neural networks was the discovery of lottery tickets. Subgraphs of initializations that are correlated to better positioning in the loss manifold can be discovered through pruning of a trained network. This was investigated after a plethora of empirical pruning results, that on mulitple state-of-the-art models...","categories": [],
        "tags": ["super-masks","research"],
        "url": "/SuperMasks/",
        "teaser":null}]
