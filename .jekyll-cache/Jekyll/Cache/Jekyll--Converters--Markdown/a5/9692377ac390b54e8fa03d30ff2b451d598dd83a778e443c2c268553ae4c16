I"ö<p>Retinanet is a near state-of-the-art object detector that using a simple adaptive weighting scheme, helps bridge some of the gap between one and two stage object detectors by dealing with inherent class imbalance from the large background set constructed by the anchoring process. Specifically they use Focal Loss</p>

<script type="math/tex; mode=display">FL(p) \propto - (1-p)^\gamma log(p)</script>

<p>The usage of <script type="math/tex">(1-p)^\gamma</script> down weights the categorical cross entropy loss along with how high <script type="math/tex">p</script> is. This is parameterized by <script type="math/tex">\gamma</script>, where the higher it is, the greater the down weighting is. In their paper they make it clear this choice of adaptive weighting is arbitrary and could be replaced with other schemes.</p>

<p>The question I pose with this, is does this accomplish their goal? (I asked this a while back in a <a href="https://ai.stackexchange.com/questions/13755/does-retina-nets-focal-loss-accomplish-its-goal">stack exchange question</a>). Given the lack of response in that post, I decided to investigate this myself. I propose another simple adaptive weighting scheme, but I propose applying the approach onto the gradient directly. I do this because most optimizers use a form of gradient descent <script type="math/tex">\theta \leftarrow \theta - \nabla_{\theta}L</script>, and so masking the gradient, directly masks how much is learned by each element. I call this set of losses, <em>focal gradient loss</em></p>

<script type="math/tex; mode=display">FGL(p) \propto - StopGradient((1-p)^\gamma) \ log(p)</script>

<h3 id="comparing-the-objective-functions">Comparing the objective functions</h3>
<p>Its hard to understand it comparitively with the <script type="math/tex">StopGradient</script> function, so first lets look at their gradients</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
    \dot{FL(p)} &= - \frac{(1-p)^\gamma }{p} + \gamma (1-p)^{\gamma - 1} \ log(p)\\
    &= -(1-p)^\gamma \ \dot{CE(p)} + \gamma (1-p)^{\gamma - 1} \ log(p)\\
    &= -(1-p)^\gamma \ \dot{CE(p)} + R(p) \\
    \dot{FGL(p)} &= - \frac{(1-p)^\gamma }{p} \\
    &= -(1-p)^\gamma \ \dot{CE(p)}  \
\end{aligned} %]]></script>

<p>Looking at these, we see the focal-loss is a masked version of adaptive weighting plus a residual, <script type="math/tex">R(p)</script>. This is the differentiating factor in the two appraoches. We can rewrite the residual as <script type="math/tex">R(p) = -\gamma * FL_{\gamma -1}(p)</script>. It is difficult to say what this does in the optimization perspective, but in terms of sheer magnitude it will always increase the gradient (because the gradient is always negative), moreso if a loss variant is high. This is difficult to describe intuitively but it may be be seen as an additive adaptive bias. This actually increases the spread if you do the math out which may be seen as a benefit but difficult to say for certain, which is why I perform this experiment.</p>

<p align="center">
  <img src="/images/FocalGradientLoss/focal_gradients.png" />
</p>

<p>Even though stop_gradient makes it not easily comparable in the loss space, we can integrate its gradient to see its effective equivalent. For values of <script type="math/tex">\gamma</script> that arent integers, the integral becomes alot more complicated, so for ease lets just look at the integer case</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
    L(p) &= \int_{z=1}^p -\frac{(1-z)^\gamma}{z} dz\\
    &= -log(p) - \sum_{i=1}^\gamma {\gamma \choose i}\frac{(-p)^i - (-1)^i}{i} \
\end{aligned} %]]></script>

<p>Note that the bottom bound of the integral is arbitrary, because it only effects the constant, but since we set <script type="math/tex">FGL(1)=0</script> from the stop_gradient formulation, I use <script type="math/tex">z=1</script> to keep it consistent. Now plotting these we see the following</p>

<p align="center">
  <img src="/images/FocalGradientLoss/focal_objectives.png" />
</p>

<p>As you can see, focal gradient loss is always lower than the respective focal loss which is understandable based on the spread differential caused by the Residual.</p>

<h3 id="experiment">Experiment</h3>
<p>Due to computational reasons, we will test on only the setting of <script type="math/tex">\gamma = 2</script> for 15 epochs on PASCAL VOC 2012. We use the keras-retinanet open source package to do this. Only adjustments I add to the repo is make the custom loss function, and I adjust the training to work with Horovod for more efficient parallelization (I will take any speedups I can get :D)</p>

<p align="center">
  <img src="/images/FocalGradientLoss/fl_vs_fgl.png" />
</p>

<p>Looking at the results we see they perform similarly. Focal Gradient Loss actually does have a slightly better mAP, but this could be due to noise produced in the training procedure. There is also more noise in general in the Focal Gradient Lossâ€™s training compared to Focal Lossesâ€“ this may be due to the lack of that additive bias. From this singular (and incomplete) experiment there isnt enough to make conclusions, but if I were forced to do so, I would say the perform similarly but Focal Gradient seems less robust. <br />
I will hopefully be adding more experiments in the future. Stay tuned.</p>

:ET